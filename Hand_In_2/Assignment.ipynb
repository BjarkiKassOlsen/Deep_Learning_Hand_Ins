{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "\n",
    "This assignment has 2 parts. The evaluation of this assignment will be performed as an overall review of the hand-in, by your TA. To pass the assignment, the TA must be convinced that you have understood all portions of the curriculum which are covered by the assignment.\n",
    "\n",
    "We expect you to hand in individual assignments, but you are allowed to discuss the questions with each other. **This means that you need to write your own answers, and your own, personal code, for all questions.**\n",
    "\n",
    "Besides getting the correct answers, solutions will also be judged based on clarity, efficiency and brevity. \n",
    "\n",
    "The assignment is due on Thursday 9th of November, at 11 pm. We expect each student to upload a pdf of their answers (written in whatever language your TA can read). Same expectations apply regarding formatting, as did for assignment 1. \n",
    "\n",
    "When preparing your notebook, you may assume that the data files accompanying this assignment are placed in the same folder as the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "**a: NLP**\n",
    "Please discuss the recent trend of rapidly increasing sizes of NLP architectures. \n",
    "\n",
    "**b: Transfer Learning**\n",
    "Classify the following example of transfer learning. More exactly, what are the domains and tasks, and which are being changed?\n",
    "\n",
    "Source: Using a step counter to monitor exercise in healthy people.\n",
    "\n",
    "Target: Using a step counter to indicate recovery progression in a patient\n",
    "\n",
    "**c: Attention**\n",
    "\n",
    "Assume dotproduct attention, and that the hidden states of the encoder layer are [0,1,4],[-1,1,2],[1,1,1],[2,1,1]. If the activation for the previous decoder is [0.1,1,-2], what is the attention-context vector?\n",
    "\n",
    "**d: Transformers**\n",
    "\n",
    "Explain the 'positional encoding' step for transformers. Why is it done, how is it done?\n",
    "\n",
    "**e: Bounding box detection:**\n",
    "Given a dataset with two classes; cats and dogs, and the following detections:\n",
    "\n",
    "TP = True positive\n",
    "FP = False positive\n",
    "\n",
    "cat_det = [TP, FP, TP, FP, TP]\n",
    "pred_scores_cat = [0.7, 0.3, 0.5, 0.6, 0.55]\n",
    "\n",
    "dog_det = [FP, TP, TP, FP, TP, TP]\n",
    "pred_scores_dog = [0.4, 0.35, 0.95, 0.5, 0.6, 0.7]\n",
    "\n",
    "There are in total 3 cats and 4 dogs in the images.\n",
    "\n",
    "Calculate the mean average precision (mAP)\n",
    "\n",
    "**f: Semantic segmentation - FCN 1:**\n",
    "Given an image sized 1024x768x3 (width x height x channels), with 7 classes. What is the size of the target image if targets are one-hot encoded?\n",
    "\n",
    "**g: Semantic segmentation - FCN 2:**\n",
    "What is a fully-convolutional network? When is it useful?\n",
    "\n",
    "**h:Residual Networks:**\n",
    "Explain residual layers and their advantage. \n",
    "\n",
    "**i: Intersection-Over-Union**\n",
    "Calculate the intersection over union in for these four bounding-boxes and target bounding boxes:\n",
    "\n",
    "***IMAGE MISSING***\n",
    "\n",
    "**j: Variational autoencoders:**\n",
    "What are the strengths of a variational autoencoder (VAE) compared to an autoencoder (AE)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Below is attached a script for generating a data set to learn translation of dates between multiple human readable and a machine readable format (ISO 8601). \n",
    "\n",
    "<u>Task</u>: Using an encoder-decoder setup, perform translation from human readable to machine readable formats. Please express the performance of your trained network in terms of average accuracy of the translated output (so, accuracy on a per-character basis). \n",
    "\n",
    "<u>Restriction</u>: we specifically demand that your presented solution does not include an attention layer. \n",
    "\n",
    " Despite this restriction, the task can be solved in numerous different ways. Here are some examples of solutions of similar problems, for inspiration: \n",
    "\n",
    "https://jscriptcoder.github.io/date-translator/Machine%20Translation%20with%20Attention%20model.html\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "Script for generating data set:\n",
    "\n",
    "<a href=\"https://brightspace.au.dk/content/enforced/108586-LR25340/dateTrans_student1.py?_&d2lSessionVal=sNbp9AfUWwMNBARne3KWu9NsG\"> date_trans</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
